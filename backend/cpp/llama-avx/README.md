# Backend llama.cpp avec support AVX

Ce rÃ©pertoire contient la configuration pour compiler le backend llama.cpp avec les optimisations AVX.

## ðŸ“‹ AperÃ§u

Ce backend utilise une version modifiÃ©e de llama.cpp optimisÃ©e pour les processeurs supportant les instructions AVX. Il gÃ©nÃ¨re un serveur gRPC (`grpc-server`) qui permet Ã  LocalAI de communiquer avec les modÃ¨les de langage.

## ðŸ”§ Fichiers principaux

### Scripts de build
- `prepare.sh` - Script de prÃ©paration de l'environnement de build
- `Makefile` - Configuration de compilation principale

### Sources
- `grpc-server.cpp` - Code source du serveur gRPC principal
- `mtmd-stub.h` / `mtmd-stub.cpp` - Stubs pour le support multimodal (dÃ©sactivÃ©)
- `grpc-server-CMakeLists.txt` - Configuration CMake pour le serveur gRPC

### Configuration
- `CMakeLists.txt` - Configuration CMake globale
- `patches/` - Patches spÃ©cifiques Ã  appliquer

## ðŸš€ Utilisation

### Build automatique (recommandÃ©)
Le build est automatiquement dÃ©clenchÃ© par le Makefile principal de LocalAI :
```bash
cd /fork/LocalAI
make build GO_TAGS="tts"
```

### Build manuel
Pour compiler manuellement ce backend :
```bash
cd /fork/LocalAI/backend/cpp/llama-avx

# Nettoyer l'environnement
make purge

# PrÃ©parer l'environnement
bash prepare.sh

# Compiler le serveur gRPC
make grpc-server
```

## ðŸ” Structure aprÃ¨s build

```
llama-avx/
â”œâ”€â”€ llama.cpp/                    # Sources llama.cpp clonÃ©es
â”‚   â”œâ”€â”€ build/                    # Artifacts de compilation
â”‚   â”‚   â””â”€â”€ bin/grpc-server       # ExÃ©cutable final
â”‚   â””â”€â”€ tools/grpc-server/        # Sources prÃ©parÃ©es pour gRPC
â”œâ”€â”€ grpc-server                   # ExÃ©cutable copiÃ© (rÃ©sultat final)
â””â”€â”€ [fichiers de configuration]
```

## âš™ï¸ Variables d'environnement

### Variables CMake utilisÃ©es
- `BUILD_SHARED_LIBS=OFF` - Build statique
- `GGML_NATIVE=OFF` - DÃ©sactive les optimisations natives
- `GGML_AVX=on` - Active le support AVX
- `GGML_AVX2=off` - DÃ©sactive AVX2
- `GGML_AVX512=off` - DÃ©sactive AVX-512
- `GGML_FMA=off` - DÃ©sactive FMA
- `GGML_F16C=off` - DÃ©sactive F16C
- `LLAMA_CURL=OFF` - DÃ©sactive libcurl

### Variables Makefile
- `LLAMA_VERSION` - Version/commit de llama.cpp Ã  utiliser
- `CMAKE_ARGS` - Arguments supplÃ©mentaires pour CMake
- `BUILD_TYPE` - Type de build (vide pour AVX)

## ðŸ”§ Processus de prÃ©paration

Le script `prepare.sh` effectue les Ã©tapes suivantes :

1. **Initialisation** : Copie des sources llama.cpp depuis le rÃ©pertoire `llama/`
2. **PrÃ©paration des sources** : 
   - Copie des fichiers nÃ©cessaires vers `llama.cpp/tools/grpc-server/`
   - Application des corrections d'headers
   - Copie des dÃ©pendances vendor (minja)
3. **Corrections automatiques** :
   - DÃ©sactivation du code multimodal
   - Suppression de la fonction main conflictuelle
   - Correction des chemins d'inclusion
4. **GÃ©nÃ©ration des assets** :
   - CrÃ©ation des fichiers protobuf
   - Copie des assets web
   - GÃ©nÃ©ration du shim UTF8

## ðŸ› ProblÃ¨mes connus et solutions

### 1. Erreurs de compilation avec multimodal
**SymptÃ´me** : Erreurs liÃ©es aux types `mtmd_*`
**Solution** : Le code multimodal est automatiquement dÃ©sactivÃ© par `prepare.sh`

### 2. DÃ©pendances manquantes
**SymptÃ´me** : `undefined reference` lors du linkage
**Solution** : Tous les fichiers sources nÃ©cessaires sont automatiquement inclus

### 3. Headers introuvables
**SymptÃ´me** : `No such file or directory` pour minja ou autres
**Solution** : Les dÃ©pendances vendor sont copiÃ©es automatiquement

## ðŸ“Š Performance

Cette configuration est optimisÃ©e pour :
- Processeurs x86-64 avec support AVX
- Performance CPU sans accÃ©lÃ©ration GPU
- DÃ©ploiements sur serveurs standards

**Benchmark typique** :
- Tokens/sec : Variable selon le modÃ¨le et le CPU
- MÃ©moire : ~2-8GB selon la taille du modÃ¨le
- CPU : Utilisation optimisÃ©e des instructions AVX

## ðŸ”„ Maintenance

### Mise Ã  jour de llama.cpp
1. Modifier `LLAMA_VERSION` dans le Makefile principal
2. Tester la compilation : `make purge && make grpc-server`
3. VÃ©rifier la compatibilitÃ© des stubs MTMD
4. Mettre Ã  jour cette documentation si nÃ©cessaire

### Ajout de nouvelles dÃ©pendances
1. Modifier `grpc-server-CMakeLists.txt`
2. Mettre Ã  jour `prepare.sh` si des fichiers doivent Ãªtre copiÃ©s
3. Tester la compilation complÃ¨te

### Debug
Pour diagnostiquer les problÃ¨mes :
```bash
# Build verbose
make grpc-server VERBOSE=1

# Logs dÃ©taillÃ©s
bash -x prepare.sh

# VÃ©rification des dÃ©pendances
ldd grpc-server
```

## ðŸ“ Changelog

### Version actuelle (Juillet 2025)
- âœ… RÃ©solution des conflits de dÃ©finition ggml_log_level
- âœ… Ajout de toutes les dÃ©pendances manquantes  
- âœ… Support automatique des assets minja
- âœ… DÃ©sactivation propre du code multimodal
- âœ… Build reproductible et maintenable

---

> **Note** : Ce README doit Ãªtre synchronisÃ© avec `/fork/LocalAI/backend/cpp/BUILD_FIXES.md` lors de modifications importantes.
